# PLDA Quick Start #
## Introduction ##

PLDA must be run in Linux enviroment with C++ compiler and MPI library installed.

## Installation ##

Download the package `plda-omp.tar.gz`

	tar xzvf plda-omp.tar.gz
	cd plda-omp
	make

This package includes MPI/OpenMP hybrid parallel version of LDA and its pre- and post-processing tools only. There binary file will be generated in the directory: omp_lda, tools/prepocess and tools/postprocess. To us infer application, please download and compile the original version 3.1 of PLDA from https://code.google.com/p/plda/downloads/detail?name=plda-3.1.tar.gz

## A simple training and testing ##

There is a file `test_data.txt` in the package under `plda-omp/testdata` directory.

**Training:**

	tools/preprocess --input testdata/test_data.txt --output /tmp/index_test_data.txt --vocab_file /tmp/vocab_file.txt --threshold 5

	mpirun -n 4 -env OMP_NUM_THREADS=4 ./omp_lda --num_topics 2 --alpha 0.1 --beta 0.01 --training_data_file testdata/index_test_data.txt --model_file /tmp/index_lda_model.txt --total_iterations 150

	tools/postprocess --vocab_file /tmp/vocab_file.txt --input_model /tmp/index_lda_model.txt --output_model /tmp/lda_model.txt

After training completes, a file `/tmp/lda_model.txt` is generated which stores the training result. Each line is the topic distribution of a word. The first element is the word string, the its occurrence count within each topic. A Python script `view_model.py` is used to convert the model to a readable text.

**Infer unseen documents:**

	./infer --alpha 0.1 --beta 0.01 --inference_data_file testdata/test_data.txt --inference_result_file /tmp/inference_result.txt --model_file /tmp/lda_model.txt --total_iterations 15 --burn_in_iterations 10

## Command-line flags ##
**Pre-processing flags:**

- `input`: orginal training data.
- `output`: processed training data that replace word with its unique index number.
- `vocab_file`: vocabulary file to store word/unique index number pairs.
- `threshold`: threshold value, the words whose occurrence numbers are less than this value will be omitted. 

**Post-processing flags:**

- `vocab_file`: the vocabulary file generated by pre-processing.
- `input_model`: training model using unique index number.
- `output_model`: training model using word.

**Training flags:**

- `alpha`: Suggested to be `50/number_of_topics`
- `beta`: Suggested to be `0.01`
- `num_topics`: The total number of topics.
- `total_iterations`: The total number of GibbsSampling iterations.
- `burn_in_iterations`: After `--burn_in_iterations` iteration, the model will be almost converged. Then we will average models of the last `(total_iterations-burn_in_iterations)` iterations as the final model. **This only takes effect for single processor version**. For example: you set `total_iterations` to 200, you found that after 170 iterations, the model is almost converged. Then you could set `burn_in_iterations` to 170 so that the final model will be the average of the last 30 iterations.
- `model_file`: The output file of the trained model.
- `training_data_file`: The training data.

## Inferring flags ##

- `alpha` and `beta` should be the same with training.
- `Total_iterations`: The total number of GibbsSampling iterations for an unseen document to determine its word topics. This number needs not be as much as training, usually tens of iterations is enough.
- `burn_in_iterations`: For an unseen document, we will average the `document_topic_distribution` of the last `(total_iterations-burn_in_iterations)` iterations as the final `document_topic_distribution`.